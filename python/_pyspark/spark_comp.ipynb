{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_name = \"sales_data\"\n",
    "\n",
    "spark = SparkSession.builder.appName(session_name).getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 35_000\n",
    "\n",
    "current_date = datetime.date.today()\n",
    "current_time = datetime.datetime.now()\n",
    "\n",
    "pval = [.05,.25,.3,.05,.1,.1,.1,.05]\n",
    "\n",
    "np.random.RandomState(seed=7)\n",
    "\n",
    "df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        'trans_dates':np.random.choice(np.arange('2010-01-01', '2021-01-01', dtype='datetime64[D]'), size=N),\n",
    "        'LOCATIONS':np.random.choice(['CA','TX','NY','OH', 'FL'], size=(N,), p=[.1,.2,.3,.35,.05]),\n",
    "        'employees':np.random.choice(['Aiden', 'Leslie', 'Ian', 'Harlan', 'Maeve', 'Kendra', 'Allen', 'Jo'], size=(N,), p=pval),        \n",
    "        'Sales/Hrs':np.random.choice(np.arange(1, 17, 1), size=N),\n",
    "        'sales tot$':np.random.normal(loc=1000, scale=20, size=N),\n",
    "        'doc_date':current_date.strftime(\"%m/%d/%Y\"),\n",
    "    })\n",
    "\n",
    "df['actuals'] = np.where(df['trans_dates'] <= current_date.strftime(\"%Y-%m-%d\"), True, False)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sales tot$'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cols(x=str):\n",
    "    x = x.lower().replace(\",\",\"\").replace(\",\",\"\").replace(\":\",\"_\").replace(\"$\",\"\").replace(\"-\",\"\").replace(\"/\",\"_\")\n",
    "    x = x.replace(\" \",\"_\")\n",
    "    x = x.replace(\"___\",\"_\")\n",
    "    x = x.replace(\"__\",\"_\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = list(map(clean_cols, df.columns))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_types = {\n",
    "    'object':'string',\n",
    "    'int64':'integer',\n",
    "    'int32':'integer',\n",
    "    'int16':'integer',\n",
    "    'float64':'double',\n",
    "    'float32':'double',\n",
    "    'datetime64[ns]':'date'\n",
    "}\n",
    "\n",
    "for i, x in enumerate(df.dtypes):\n",
    "    col_name = df.dtypes.index[i]\n",
    "    col_type = str(df.dtypes.values[i])\n",
    "    \n",
    "    if 'date' in col_name:\n",
    "        print(col_name, 'date', end='')\n",
    "    else:\n",
    "        print(col_name, sql_types.get(col_type, 'string'), end='')\n",
    "    print(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.parquet(\"sales_data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.withColumnRenamed('sales_tot', 'sales_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.withColumn('doc_date', to_timestamp(col('doc_date'), \"m/d/yyyy\"))\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.datetime.now()\n",
    "current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.withColumn('current_date', lit(current_date))\n",
    "ds.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.withColumn('sales_total', round(col('sales_total') ,2))\n",
    "ds.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_sum(e):\n",
    "    if e == \"Maeve\": return 'Group1'\n",
    "    elif e == \"Harlan\": return 'Group2'\n",
    "    else: return 'Group3'\n",
    "    \n",
    "emp_sum = udf(emp_sum, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.withColumn('employee_summary', emp_sum('employees'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sample(False, fraction=0.1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sort('trans_dates').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sort('trans_dates').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.filter(ds['locations']=='CA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupBy('employee_summary').agg({'sales_total':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['locations']\n",
    "aggs = ['sales_total']\n",
    "\n",
    "func = [sum, mean, max, min]\n",
    "\n",
    "expr = [f(col(c)) for f in func for c in aggs]\n",
    "\n",
    "ds.groupBy(*groups).agg(*expr).orderBy('locations').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupBy('locations').pivot('employees').agg(sum('sales_total')).orderBy('locations').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.filter(ds['employees']=='Aiden').groupBy('locations').pivot('actuals').agg(sum('sales_total')).orderBy('locations').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "locations = (\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI',\n",
    "    'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI',\n",
    "    'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC',\n",
    "    'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT',\n",
    "    'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'PR', 'GU', 'VI', 'AS', 'MP',\n",
    "    'DC'\n",
    ")\n",
    "\n",
    "stname = (\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "    'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas',\n",
    "    'Kentucky', 'Louisiana',  'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
    "    'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "    'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
    "    'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas',\n",
    "    'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming',\n",
    "    'Puerto Rico', 'Guam', 'U.S. Virgin Islands', 'American Samoa', 'Northern Mariana Islands',\n",
    "    'District of Columbia'\n",
    ")\n",
    "\n",
    "region = (\n",
    "    'South', 'West', 'West', 'South', 'West', 'West', 'Northeast', 'South', 'South', 'South',\n",
    "    'West', 'West', 'Midwest', 'Midwest', 'Midwest', 'Midwest', 'South', 'South', 'Northeast',\n",
    "    'South', 'Northeast', 'Midwest', 'Midwest', 'South', 'Midwest', 'West', 'Midwest', 'West',\n",
    "    'Northeast', 'Northeast', 'West', 'Northeast', 'South', 'Midwest', 'Midwest', 'South',\n",
    "    'West', 'Northeast', 'Northeast', 'South', 'Midwest', 'South', 'South', 'West', 'Northeast',\n",
    "    'South', 'West', 'South', 'Midwest', 'West', 'South', 'South', 'South', 'South', 'South',\n",
    "    'South'\n",
    ")\n",
    "\n",
    "dt = sc.parallelize([Row(locations=col[0], stname=col[1], region=col[2]) for col in list(zip(locations,stname,region))]).toDF()\n",
    "\n",
    "dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.where(\"region == 'South'\").select('locations', 'stname').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj = ds.join(dt, on='locations', how='left')\n",
    "dj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.createOrReplaceTempView(\"sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sales_data LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT count(*) AS total_count, round(sum(sales_total)/1000) AS total_sales_k FROM sales_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT region, round(sum(sales_total)/1000) AS sales_k FROM sales_data GROUP BY region ORDER BY 2 DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = spark.sql(\"SELECT region, stname, employees, sales_hrs, sales_total FROM sales_data WHERE region IN ('Midwest', 'West')\").toPandas()\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['region'].value_counts(dropna=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
