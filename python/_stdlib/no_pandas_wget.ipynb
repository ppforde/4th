{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# check system information\n",
    "print('Python Information', sys.version)\n",
    "print('This is your current directory', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime libraries\n",
    "import datetime\n",
    "from datetime import time\n",
    "\n",
    "# assgin current date and time\n",
    "currentDate = datetime.date.today()\n",
    "currentTime = datetime.datetime.now()\n",
    "\n",
    "# check datetime information\n",
    "print('Today is {}'.format(currentDate))\n",
    "print('Today is', datetime.datetime.strftime(currentDate, '%m/%d/%Y'))\n",
    "print('The time is', datetime.datetime.strftime(currentTime, '%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Auto Loans](#Auto-Loans)\n",
    "1. [Baby Names](#Baby-Names)\n",
    "1. [Crappy Gifts](#Crappy-Gifts)\n",
    "1. [Solar Power](#Solar-Power)\n",
    "1. [USA](#USA)\n",
    "1. [Census Estimates](#Census-Estimates)\n",
    "1. [IMDB](#IMDB)\n",
    "1. [Gutenburg](#Gutenburg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loan_func(present_value, number_periods, interest_rate):\n",
    "    interest_rate = interest_rate/100/12\n",
    "    return round(interest_rate * present_value / (1-(1+interest_rate)** -number_periods),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Auto\n",
    "print(loan_func(25515, 72, 3.19))\n",
    "\n",
    "# Mortgage\n",
    "print(loan_func(403700, 360, 4.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = 3.19\n",
    "\n",
    "current_date = datetime.datetime.now().strftime(\"%m/%d/%Y\")\n",
    "\n",
    "loans = [\n",
    "    {'Present Value':pv,\n",
    "     'Number of Periods':np,\n",
    "     'Interest Rate':ir,\n",
    "     'Monthly Payment':loan_func(pv, np, ir),\n",
    "     'Document Date': current_date,\n",
    "    } for pv in range(22000, 30000, 1000) for np in range(12,84,12)\n",
    "]\n",
    "\n",
    "print(len(loans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in random.choices(loans, k=4):\n",
    "    pprint(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='loans.csv', mode='w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = loans[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baby Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to save files\n",
    "print('This is your current directory', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('OUTPUT')\n",
    "except FileExistsError:\n",
    "     print('The directory already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.ssa.gov/oact/babynames/state/namesbystate.zip'\n",
    "filename = wget.download(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('BABY_DATA')\n",
    "except FileExistsError:\n",
    "     print('The directory already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(filename, mode='r') as z:\n",
    "    z.extractall(path='BABY_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('BABY_DATA/*.TXT')\n",
    "pprint(files, compact=True, width=80)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['state_abbr', 'gender', 'birth_year', 'birth_name', 'total']\n",
    "row_dict_list = list()\n",
    "\n",
    "for file in files:\n",
    "    with open(file, mode='r') as f:\n",
    "        for line in f.readlines():\n",
    "            row = line.strip().split(',')\n",
    "            row_dict = dict(zip(header, row))\n",
    "            row_dict['year'] = row_dict['birth_year'] + \"-12-31\"\n",
    "            row_dict['initial'] = row_dict['birth_name'][0]\n",
    "            row_dict['length'] = len(row_dict['birth_name'])\n",
    "\n",
    "            row_dict_list.append(row_dict)\n",
    "            \n",
    "print(\"Number of records in collection:\", len(row_dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in (random.choices(row_dict_list, k=6)):\n",
    "    pprint(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='babies.csv', mode='w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = row_dict_list[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(row_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del row_dict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crappy Gifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = ['Blinking Robot', '27in televsion', 'Laptop', '8 x 10 Rug',\n",
    "            '14pc Cutlery Set', 'Stuffed Alien - Grey', 'Mint Creme Cookies',\n",
    "            'Kale Chips', 'Baseball Cap', 'Shoes', 'XL Hoodie']\n",
    "\n",
    "employees = ['Hattie', 'Jes', 'Kira']\n",
    "\n",
    "locations = ['NY', 'TX', 'CA', 'OH', 'MI', 'PR']\n",
    "\n",
    "clients = ['ULTA', 'ALK', 'TM', 'BUD', 'CXO', 'ACN', 'MA', 'WHR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.date(2020,1,1)\n",
    "\n",
    "transactions = [\n",
    "    {'product':random.choice(products),\n",
    "     'employee':e,\n",
    "     'location':random.choice(locations),\n",
    "     'client':random.choice(clients),\n",
    "     'quantity':random.randrange(0,1000),\n",
    "     'sales_rate':round(random.random(),2),\n",
    "     'sales_date':(start_date + datetime.timedelta(d)).strftime(format=\"%Y-%m-%d\"),\n",
    "     'sales_total':0,\n",
    "     'check':False,\n",
    "    } for e in employees for d in range(0,(366+365))\n",
    "]\n",
    "\n",
    "for row in transactions:\n",
    "    row['sales_total'] = round((row['quantity'] * row['sales_rate']),2)\n",
    "    if row['quantity'] > 975:\n",
    "        row['check'] = True            \n",
    "\n",
    "print(len(transactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in random.choices(transactions, k=3):\n",
    "    pprint(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='crappy_gifts.csv', mode='w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = transactions[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solar Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://data.ny.gov/api/views/3x8r-34rs/rows.csv?accessType=DOWNLOAD&sorting=true'\n",
    "\n",
    "filename = wget.download(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, newline=\"\", encoding='utf-8') as f:\n",
    "    solar_data = list(csv.reader(f, delimiter = \",\"))\n",
    "    solar_header = solar_data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "print(list(map(nums, [5,.5,0.5,'5',5e3,'5e3','five'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(random.choice(solar_data)):\n",
    "    print(i, solar_header[i], \">>>\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, z in enumerate(solar_header):\n",
    "    data_sets = [x[e] for x in solar_data]\n",
    "    print(\"Index # {} for {} has {:,} records\".format(e,z,len(set(data_sets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(Counter(x[13] for x in solar_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(Counter(x[9] for x in solar_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_json = []\n",
    "\n",
    "with open (filename, 'r') as f:\n",
    "    reader = csv.DictReader(f, delimiter=',')\n",
    "    header = reader.fieldnames\n",
    "    for row in reader:\n",
    "        \n",
    "        if row['Project Status'] == 'Complete':\n",
    "        \n",
    "            # split into a list\n",
    "            row['Program Type'] = row['Program Type'].split(\"/\")\n",
    "\n",
    "            app_list = row['Location 1'].split(\"\\n\")\n",
    "            addr = app_list[0]\n",
    "            lat = nums(app_list[-1].split(\",\")[0].replace(\"(\",\"\"))\n",
    "            lon = nums(app_list[-1].split(\",\")[-1].strip().replace(\")\",\"\"))\n",
    "            row['Location 1'] = dict(zip(['Address', 'Latitude', 'Longitude'], [addr, lat, lon]))        \n",
    "\n",
    "            # covert numeric fields\n",
    "            for col in ['Total Inverter Quantity', 'Total Nameplate kW DC', 'Total PV Module Quantity',\n",
    "                        'Expected KWh Annual Production', 'Project Cost', '$Incentive']:    \n",
    "                row[col] = nums(row[col])\n",
    "                \n",
    "            # replace columns        \n",
    "            row['Incentive'] = row['$Incentive']\n",
    "\n",
    "            # delete column\n",
    "            del row['$Incentive']\n",
    "\n",
    "            solar_json.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(random.choice(solar_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='solar.csv', mode='w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = solar_json[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(solar_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del solar_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = (\n",
    "    1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
    "    25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,\n",
    "    44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 72, 66, 78, 60, 69,\n",
    ")\n",
    "\n",
    "stname = (\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n",
    "    'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas',\n",
    "    'Kentucky', 'Louisiana',  'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
    "    'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey',\n",
    "    'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
    "    'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas',\n",
    "    'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming',\n",
    "    'Puerto Rico', 'Guam', 'U.S. Virgin Islands', 'American Samoa', 'Northern Mariana Islands',\n",
    ")\n",
    "\n",
    "stabbr = (\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI',\n",
    "    'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI',\n",
    "    'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC',\n",
    "    'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT',\n",
    "    'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'PR', 'GU', 'VI', 'AS', 'MP',\n",
    ")\n",
    "\n",
    "region = (\n",
    "    'South', 'West', 'West', 'South', 'West', 'West', 'Northeast', 'South', 'South', 'South',\n",
    "    'West', 'West', 'Midwest', 'Midwest', 'Midwest', 'Midwest', 'South', 'South', 'Northeast',\n",
    "    'South', 'Northeast', 'Midwest', 'Midwest', 'South', 'Midwest', 'West', 'Midwest', 'West',\n",
    "    'Northeast', 'Northeast', 'West', 'Northeast', 'South', 'Midwest', 'Midwest', 'South',\n",
    "    'West', 'Northeast', 'Northeast', 'South', 'Midwest', 'South', 'South', 'West', 'Northeast',\n",
    "    'South', 'West', 'South', 'Midwest', 'West', 'South', 'South', 'South', 'South', 'South',\n",
    ")\n",
    "\n",
    "division = (\n",
    "    'East South Central', 'Pacific', 'Mountain', 'West South Central', 'Pacific',\n",
    "    'Mountain', 'New England', 'South Atlantic', 'South Atlantic', 'South Atlantic',\n",
    "    'Pacific', 'Mountain', 'East North Central', 'East North Central', 'West North Central',\n",
    "    'West North Central', 'East South Central', 'West South Central', 'New England',\n",
    "    'South Atlantic', 'New England', 'East North Central', 'West North Central', \n",
    "    'East South Central', 'West North Central', 'Mountain', 'West North Central', 'Mountain',\n",
    "    'New England', 'Middle Atlantic', 'Mountain', 'Middle Atlantic', 'South Atlantic',\n",
    "    'West North Central', 'East North Central', 'West South Central', 'Pacific',\n",
    "    'Middle Atlantic', 'New England', 'South Atlantic', 'West North Central',\n",
    "    'East South Central', 'West South Central', 'Mountain', 'New England',\n",
    "    'South Atlantic', 'Pacific', 'South Atlantic', 'East North Central', 'Mountain',\n",
    "    'South Atlantic', 'Pacific','South Atlantic', 'Pacific', 'Pacific',\n",
    ")\n",
    "\n",
    "seats = (\n",
    "    7,1,9,4,53,7,5,1,27,14,2,2,18,9,4,4,6,6,2,8,9,14,\n",
    "    8,4,8,1,3,4,2,12,3,27,13,1,16,5,5,18,2,7,1,9,36,4,\n",
    "    1,11,10,3,8,1,0,0,0,0,0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ('st_fips', 'st_name', 'st_abbr', 'st_regs', 'st_divs', 'st_seats')\n",
    "states_json = []\n",
    "\n",
    "for values in zip(fips, stname, stabbr, region, division, seats):\n",
    "    row = dict(zip(keys, values))\n",
    "    states_json.append(row)\n",
    "    \n",
    "for row in states_json[0:5]:\n",
    "    pprint(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='united_states.csv', mode='w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = states_json[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(states_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://www2.census.gov/programs-surveys/popest/datasets/2010-{0}/national/totals/nst-est{0}-alldata.csv'\n",
    "\n",
    "year = '2019'\n",
    "\n",
    "print(URL.format(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = wget.download(URL.format(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, newline=\"\", encoding='latin-1') as f:\n",
    "    estimate_data = list(csv.reader(f, delimiter = \",\"))\n",
    "    estimate_header = estimate_data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(estimate_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(estimate_header):\n",
    "    print(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(zip(estimate_header, random.choice(estimate_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, z in enumerate(estimate_header):\n",
    "    data_sets = [x[e] for x in estimate_data]\n",
    "    print(\"Index # {} for {} has {:,} records\".format(e,z,len(set(data_sets))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nums(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "print(list(map(nums, [5,.5,0.5,'5',5e3,'5e3','five'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_header.index(\"POPESTIMATE2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "value = 16\n",
    "\n",
    "pivot_data = sorted(set(x[index] for x in estimate_data))\n",
    "\n",
    "for z in pivot_data:\n",
    "    pivot_totals = []\n",
    "    for x in estimate_data:\n",
    "        if x[index] == z:\n",
    "            pivot_totals.append(nums(x[value]))\n",
    "    print(\"{} = {:,.2f}\".format(z, sum(pivot_totals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_cols = estimate_header[7:17]\n",
    "\n",
    "for p in pivot_cols:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_cols = [x for x in estimate_header if x.startswith(\"POPEST\")]\n",
    "\n",
    "for p in pivot_cols:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('POPESTIMATE2019'[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = {p: pivot_cols[i][-4:]+\"-12-31\" for i, p in enumerate(pivot_cols)}\n",
    "\n",
    "pprint(periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates_json = []\n",
    "with open(filename) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        if row['SUMLEV'] == '040':\n",
    "            for columns in pivot_cols:\n",
    "                estimates_json.append(\n",
    "                    dict(                 \n",
    "                        scenario=columns,\n",
    "                        year_end=periods[columns],\n",
    "                        summary_level=row['SUMLEV'],\n",
    "                        location=row['NAME'],\n",
    "                        region=row['REGION'],\n",
    "                        division=row['DIVISION'],\n",
    "                        population=row[columns],\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in random.choices(estimates_json, k=4):\n",
    "    pprint(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='estimates.csv', mode='w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames = estimates_json[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(estimates_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del estimates_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://datasets.imdbws.com/title.basics.tsv.gz'\n",
    "\n",
    "filename = wget.download(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(filename, mode='r') as f_in, open(filename.replace('.gz',''), mode='wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompressed_file = filename.replace('.gz','')\n",
    "\n",
    "print(uncompressed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(uncompressed_file, mode='r', newline='', encoding='utf-8') as f:\n",
    "    imdb_data = list(csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE, ))\n",
    "    imdb_header = imdb_data.pop(0)\n",
    "    \n",
    "print(len(imdb_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(imdb_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, z in enumerate(imdb_header):\n",
    "    try:\n",
    "        data_sets = [x[e] for x in imdb_data]\n",
    "        print(\"Index #{} for {} has {:,} records\".format(e, z, len(set(data_sets))))\n",
    "    except:\n",
    "        print(\"Index #{} has a error\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(Counter(x[4] for x in imdb_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(Counter(x[1] for x in imdb_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_wars = [title for title in imdb_data if 'Star Wars' in title[2]]\n",
    "print(len(star_wars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head, row in zip(imdb_header, random.choice(star_wars)):\n",
    "    print(head, '>>>', row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://m.imdb.com/chart/top'\n",
    "\n",
    "with urllib.request.urlopen(URL) as f:\n",
    "    web_data = f.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_titles = re.findall(pattern=\"(tt\\d+)\", string=web_data)\n",
    "\n",
    "movie_titles = set(movie_titles)\n",
    "\n",
    "pprint(movie_titles, compact=True, width=132)\n",
    "print()\n",
    "print(len(movie_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(uncompressed_file, mode='r', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    with open('imdb_titles.csv', mode='w', newline='', encoding='utf-8')as fw:\n",
    "        fieldnames = reader.fieldnames\n",
    "        writer = csv.DictWriter(fw, dialect='excel', fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            if (row['isAdult'] == '0') & (row['tconst'] in movie_titles):\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gutenburg\n",
    "\n",
    "http://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://www.gutenberg.org/cache/epub/5061/pg5061.txt'\n",
    "\n",
    "book = wget.download(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(book, encoding='utf8') as f:\n",
    "    data = f.readlines()\n",
    "    \n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(data[0:20]):\n",
    "    print(i, line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(data):\n",
    "    if \"Tiny Tim\" in line:\n",
    "        print(i, \"->\", line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(book, encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(pattern, string):\n",
    "    patt = re.compile(pattern)\n",
    "    matches = patt.finditer(string)    \n",
    "    return list(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny Tim?\n",
    "for d in regex(r'Tiny Tim', data):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentences with \"Tiny Tim\"\n",
    "get_sentence = re.findall(r\"[^.]*Tiny Tim[^.]*\\.\", data)\n",
    "\n",
    "print(len(get_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in get_sentence:\n",
    "    print(sentence.strip())\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
